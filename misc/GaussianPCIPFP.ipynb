{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq_XE6Ei9Kqq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#IPFP algo (for comparison)\n",
        "def ipfp(M, a, b, its = 40):\n",
        "  for _ in range(its):\n",
        "    row_margs = M.sum(dim = 1, keepdim= True)\n",
        "    M = M * (a.unsqueeze(1) / row_margs)\n",
        "    col_margs = M.sum(dim = 0, keepdim= True)\n",
        "    M = M * (b.unsqueeze(0) / col_margs)\n",
        "  P = M\n",
        "  return P\n",
        "\n",
        "def G1(x1, R, R_cumsum):\n",
        "    l = R.shape[0]\n",
        "    x1 = x1.to(R.device).float()\n",
        "    R = R.to(dtype=torch.float32)\n",
        "    R_cumsum = R_cumsum.to(dtype=torch.float32)\n",
        "\n",
        "    if x1.dim() == 1:\n",
        "        x1 = x1.unsqueeze(-1)\n",
        "\n",
        "    vals = torch.zeros((x1.shape[0], 1), dtype=torch.float32, device=x1.device)\n",
        "    ml = (x1 == l)\n",
        "    vals[ml] = 1.0\n",
        "\n",
        "    fl = torch.floor(x1)\n",
        "    fl_long = fl.long()\n",
        "\n",
        "    not_l_mask = (~ml).squeeze(-1) & (fl_long.squeeze(-1) != 0)\n",
        "    st = torch.zeros((x1.shape[0], 1), dtype=torch.float32, device=x1.device)\n",
        "\n",
        "    idx = fl_long[not_l_mask] - 1\n",
        "    st[not_l_mask] = R_cumsum[idx]\n",
        "    vals[~ml] = st[~ml] + R[fl_long[~ml].squeeze(-1)] * (x1[~ml] - fl[~ml])\n",
        "\n",
        "    return vals\n",
        "\n",
        "def G2(x2, C, C_cumsum):\n",
        "    l = C.shape[0]\n",
        "    x2 = x2.to(C.device).float()\n",
        "    C = C.to(dtype=torch.float32)\n",
        "    C_cumsum = C_cumsum.to(dtype=torch.float32)\n",
        "\n",
        "    if x2.dim() == 1:\n",
        "        x2 = x2.unsqueeze(-1)\n",
        "\n",
        "    vals = torch.zeros((x2.shape[0], 1), dtype=torch.float32, device=x2.device)\n",
        "    ml = (x2 == l)\n",
        "    vals[ml] = 1.0\n",
        "\n",
        "    fl = torch.floor(x2)\n",
        "    fl_long = fl.long()\n",
        "\n",
        "    not_l_mask = (~ml).squeeze(-1) & (fl_long.squeeze(-1) != 0)\n",
        "    st = torch.zeros((x2.shape[0], 1), dtype=torch.float32, device=x2.device)\n",
        "\n",
        "    idx = fl_long[not_l_mask] - 1\n",
        "    st[not_l_mask] = C_cumsum[idx]\n",
        "\n",
        "    vals[~ml] = st[~ml] + C[fl_long[~ml].squeeze(-1)] * (x2[~ml] - fl[~ml])\n",
        "    return vals\n",
        "def inv_cdf_batch(u, cumsum, pmf):\n",
        "    device = cumsum.device\n",
        "    if u.dim() == 1: u = u.unsqueeze(-1).to(device)  # shape (N, 1)\n",
        "    idx = torch.searchsorted(cumsum, u, right=True)\n",
        "    idx = idx.clamp(max=len(pmf)-1)\n",
        "\n",
        "    below = torch.where(idx > 0, cumsum[idx - 1], torch.zeros_like(u))\n",
        "    delta = pmf[idx]\n",
        "    offset = (u - below) / delta\n",
        "    return idx.squeeze(-1) + offset.squeeze(-1)\n",
        "def F_batch(x1, x2, E, E_cumsum):\n",
        "    device = E.device\n",
        "    # E is (l, l)\n",
        "    if x2.dim() == 1: x2 = x2.unsqueeze(-1).to(device)\n",
        "    if x1.dim() == 1: x1 = x1.unsqueeze(-1).to(device)\n",
        "    m = x1.floor().long()\n",
        "    n = x2.floor().long()\n",
        "\n",
        "    m = torch.clamp(m, max=E.size(0)-1)\n",
        "    n = torch.clamp(n, max=E.size(1)-1)\n",
        "    dx = x1 - m.float()\n",
        "    dy = x2 - n.float()\n",
        "\n",
        "    def safe_gather(E_cumsum, m_idx, n_idx):\n",
        "        mask = (m_idx >= 0) & (n_idx >= 0)\n",
        "        vals = torch.zeros((m_idx.shape[0], 1), dtype=E_cumsum.dtype, device=E_cumsum.device)\n",
        "\n",
        "        if mask.any():\n",
        "            valid_indices = torch.stack([m_idx[mask], n_idx[mask]], dim=0)  # shape (2, N)\n",
        "            gathered = E_cumsum[valid_indices[0], valid_indices[1]]\n",
        "            vals[mask] = gathered.to(dtype=vals.dtype)\n",
        "\n",
        "        return vals\n",
        "\n",
        "    A = safe_gather(E_cumsum, m-1, n-1)\n",
        "    B = safe_gather(E_cumsum, m-1, n)\n",
        "    C_val = safe_gather(E_cumsum, m, n-1)\n",
        "    return A + (C_val-A)*(dx) + (B-A)*(dy) + E[m, n]*(dx)*(dy)\n",
        "\n",
        "def Cop_batch(u1, u2, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, E, E_cumsum):\n",
        "    return F_batch(inv_cdf_batch(u1, E_rows_cumsum, E_rows), inv_cdf_batch(u2, E_cols_cumsum, E_cols), E, E_cumsum)  # or pass E if not global\n",
        "\n",
        "def G(x1, x2, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E, E_cumsum):\n",
        "  return Cop_batch(G1(x1, R, R_cumsum), G2(x2, C, C_cumsum), E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, E, E_cumsum)\n",
        "\n",
        "def G_batch(E, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E_cumsum):\n",
        "  #E_cop = [G(i + 1, j + 1) - (G(i, j + 1) + G(i + 1, j) - G(i, j)) for i in range(l) for j in range(l)]\n",
        "  l = E.shape[0]\n",
        "  I, J = torch.meshgrid(torch.arange(l), torch.arange(l), indexing='ij')\n",
        "  I = I.flatten()  # shape (l^2,)\n",
        "  J = J.flatten()\n",
        "  I0, I1 = I, I+1\n",
        "  J0, J1 = J, J+1\n",
        "\n",
        "  G_11 = G(I1, J1, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E, E_cumsum)  # G(i+1, j+1)\n",
        "  G_01 = G(I0, J1, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E, E_cumsum)  # G(i, j+1)\n",
        "  G_10 = G(I1, J0, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E, E_cumsum)  # G(i+1, j)\n",
        "  G_00 = G(I0, J0, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E, E_cumsum)  # G(i, j)\n",
        "  E_cop_flat = G_11 - G_01 - G_10 + G_00\n",
        "  E_cop = E_cop_flat.view(l, l)\n",
        "  return E_cop\n",
        "\n",
        "def KLD(P1, P2):\n",
        "    P1 = torch.clamp(P1, min=1e-7)\n",
        "    P2 = torch.clamp(P2, min=1e-7)\n",
        "    return (P1 * (P1 / P2).log()).sum()\n",
        "\n",
        "def MI(E, E_rows, E_cols):\n",
        "  E_rows = torch.clamp(E_rows, min=1e-7)\n",
        "  E_cols = torch.clamp(E_cols, min=1e-7)\n",
        "  if E_rows.dim() == 1: E_rows = E_rows.unsqueeze(-1)\n",
        "  if E_cols.dim() == 1: E_cols = E_cols.unsqueeze(0)\n",
        "  E_ind = E_rows @ E_cols\n",
        "  return KLD(E, E_ind)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "divs = []\n",
        "\n",
        "def generate_sparse_E(l, skewness=1.0):\n",
        "    logits = torch.randn(l, l) * skewness\n",
        "    probs = torch.softmax(logits.flatten(), dim=0).view(l, l)\n",
        "    return probs\n",
        "def generate_sparse_RC(l, skewness=1.0):\n",
        "    logits = torch.randn(l) * skewness\n",
        "    probs = torch.softmax(logits, dim=0)\n",
        "    return probs\n",
        "def marg_entropy(V):\n",
        "  V = torch.clamp(V, min=1e-7)\n",
        "  return -(V*V.log()).sum()\n",
        "l = 10\n",
        "\n",
        "#SPECIFY TARGET MARGINALS, R AND C\n",
        "R = generate_sparse_RC(l, 6)\n",
        "C = generate_sparse_RC(l, 4)\n",
        "\n",
        "#SKEWNESS\n",
        "E = generate_sparse_E(l, 2.5)\n",
        "E_orig = E\n",
        "\n",
        "#Prelim IPFP (before SKLAR)\n",
        "E = ipfp(E, R, C, its = 4)\n",
        "E_prelim = E\n",
        "\n",
        "E_rows = E.sum(dim = 1, keepdim = False)\n",
        "E_cols = E.sum(dim = 0, keepdim = False)\n",
        "E_cols_cumsum = torch.cumsum(E_cols, dim=0)  # shape: (l,)\n",
        "E_rows_cumsum = torch.cumsum(E_rows, dim=0)\n",
        "E_cumsum = torch.cumsum(torch.cumsum(E, dim=0), dim = 1) #2d pref sum\n",
        "'''\n",
        "R = E_rows + 0.0001*torch.rand(l)\n",
        "R = R / R.sum()\n",
        "C = E_cols + 0.0001*torch.rand(l)\n",
        "C = C / C.sum()\n",
        "'''\n",
        "R_cumsum = torch.cumsum(R, dim=0)\n",
        "C_cumsum = torch.cumsum(C, dim=0)\n",
        "print(\"Current row marginals entropy: \", marg_entropy(E_rows))\n",
        "print(\"Current col marginals entropy: \", marg_entropy(E_cols))\n",
        "print(\"Target row entropy: \", marg_entropy(R))\n",
        "print(\"Target col entropy: \", marg_entropy(C))\n",
        "print(\"Original joint MI: \", MI(E_orig, E_rows, E_cols))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX22AzeM9TR8",
        "outputId": "995b1397-3f8c-4bf1-a3d1-30d1a36f0651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current row marginals entropy:  tensor(0.4957)\n",
            "Current col marginals entropy:  tensor(0.0498)\n",
            "Target row entropy:  tensor(0.4957)\n",
            "Target col entropy:  tensor(0.0498)\n",
            "Original joint MI:  tensor(12.2264)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "E_cop = G_batch(E, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E_cumsum)\n",
        "E_ipfp = ipfp(E, R, C)\n",
        "print(\"KLD (E_original, E_copula) \", KLD(E_orig, E_cop))\n",
        "print(\"KLD (E_original, E_ipfp)\", KLD(E_orig, E_ipfp))\n",
        "print(\"KLD (E_ipfp, E_copula)\", KLD(E_ipfp, E_cop), KLD(E_cop, E_ipfp))\n",
        "#print(\"KLD (E_ipfp, E_prelim)\", KLD(E_ipfp, E_prelim), KLD(E_prelim, E_ipfp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3vWSMsF9YAU",
        "outputId": "482dd218-30db-4ba6-e2ee-b993af3c91a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KLD (E_original, E_copula)  tensor(10.9968)\n",
            "KLD (E_original, E_ipfp) tensor(11.0085)\n",
            "KLD (E_ipfp, E_copula) tensor(4.7415e-07) tensor(-3.6090e-07)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  E = torch.tensor([[0.1, 0.1, 0.03, 0.02], [0.02, 0.05, 0.1, 0.08], [0.2, 0.01, 0.01, 0.08], [0.025, 0.05, 0.015, 0.01]])\n",
        "  R = torch.tensor([0.5, 0.4, 0.05, 0.05])\n",
        "  C = torch.tensor([0.3, 0.25, 0.3, 0.15 ])\n",
        "  E_rows = E.sum(dim = 1, keepdim = False)\n",
        "  E_cols = E.sum(dim = 0, keepdim = False)\n",
        "  E_cols_cumsum = torch.cumsum(E_cols, dim=0)  # shape: (l,)\n",
        "  E_rows_cumsum = torch.cumsum(E_rows, dim=0)\n",
        "  E_cumsum = torch.cumsum(torch.cumsum(E, dim=0), dim = 1) #2d pref sum\n",
        "  R_cumsum = torch.cumsum(R, dim=0)\n",
        "  C_cumsum = torch.cumsum(C, dim=0)\n",
        "\n",
        "  E_cop = G_batch(E, E_rows, E_rows_cumsum, E_cols, E_cols_cumsum, R, R_cumsum, C, C_cumsum, E_cumsum)\n",
        "  E_ipfp = ipfp(E, R, C)\n"
      ],
      "metadata": {
        "id": "VCSCfVg_9eXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E_ipfp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2rxs_nz9lWx",
        "outputId": "32a16a9d-456b-4b3f-a7d2-e0cb4995465d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2181, 0.1657, 0.0824, 0.0338],\n",
              "        [0.0325, 0.0618, 0.2048, 0.1009],\n",
              "        [0.0354, 0.0013, 0.0022, 0.0110],\n",
              "        [0.0139, 0.0212, 0.0105, 0.0043]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from scipy.optimize import bisect\n",
        "\n",
        "class MixtureOfGaussians:\n",
        "    def __init__(self, weights, mus, sigmas):\n",
        "        \"\"\"\n",
        "        weights: tensor of shape (K,), not necessarily normalized\n",
        "        mus: tensor of shape (K,)\n",
        "        sigmas: tensor of shape (K,)\n",
        "        \"\"\"\n",
        "        self.weights = F.softmax(weights, dim=0)\n",
        "        self.mus = mus\n",
        "        self.sigmas = sigmas\n",
        "        self.components = [Normal(mu, sigma) for mu, sigma in zip(mus, sigmas)]\n",
        "\n",
        "    def pdf(self, x):\n",
        "        x = torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n",
        "        return sum(w * comp.log_prob(x).exp() for w, comp in zip(self.weights, self.components))\n",
        "\n",
        "    def cdf(self, x):\n",
        "        x = torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n",
        "        return sum(w * comp.cdf(x) for w, comp in zip(self.weights, self.components))\n",
        "\n",
        "    def icdf(self, p, low=-10.0, high=10.0, tol=1e-5):\n",
        "        if isinstance(p, torch.Tensor):\n",
        "            if p.dim() == 0:\n",
        "                return torch.tensor(self._inverse_cdf_scalar(float(p), low, high, tol))\n",
        "            else:\n",
        "                return torch.tensor([self._inverse_cdf_scalar(float(pi), low, high, tol) for pi in p])\n",
        "        else:\n",
        "            return torch.tensor(self._inverse_cdf_scalar(float(p), low, high, tol))\n",
        "\n",
        "    def _inverse_cdf_scalar(self, target_p, low, high, tol):\n",
        "        def f(x):\n",
        "            return self.cdf(torch.tensor(x)) - target_p\n",
        "        return bisect(f, low, high, xtol=tol)\n"
      ],
      "metadata": {
        "id": "mTCEqVxT9mFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_ws = R       # unnormalized weights\n",
        "C_ws = C\n",
        "\n",
        "mus = torch.tensor([0.0, 2, 3, 4])           # means of Gaussians\n",
        "sigmas = torch.tensor([2.5, 2, 1, 1])        # std deviations\n",
        "gaussians = [Normal(mu, sigma) for mu, sigma in zip(mus, sigmas)]\n",
        "\n",
        "mog_r = MixtureOfGaussians(R_ws, mus, sigmas)\n",
        "mog_c = MixtureOfGaussians(C_ws, mus, sigmas)\n"
      ],
      "metadata": {
        "id": "66dOi-aV-WLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def F_dis(x1, x2, E, gaussians):\n",
        "    x1 = torch.as_tensor(x1, dtype=torch.float32)\n",
        "    x2 = torch.as_tensor(x2, dtype=torch.float32)\n",
        "    cdfs_x1 = torch.stack([g.cdf(x1) for g in gaussians])  # [k, ...]\n",
        "    cdfs_x2 = torch.stack([g.cdf(x2) for g in gaussians])  # [k, ...]\n",
        "\n",
        "    # E[i,j] * cdfs_x1[i] * cdfs_x2[j] summed over i,j\n",
        "    result = torch.einsum('ij,i...,j...->...', E, cdfs_x1, cdfs_x2)\n",
        "\n",
        "    return result\n",
        "\n",
        "#now try straight up sklar on\n",
        "orig_r = MixtureOfGaussians(E_rows, mus, sigmas)\n",
        "orig_c = MixtureOfGaussians(E_cols, mus, sigmas)\n",
        "\n",
        "def fr_sklar(x1, x2, E, gaussians):\n",
        "  x1 = torch.tensor(x1)\n",
        "  x2 = torch.tensor(x2)\n",
        "  u = mog_r.cdf(x1)\n",
        "  v = mog_c.cdf(x2)\n",
        "  x1_new = orig_r.icdf(u)\n",
        "  x2_new = orig_c.icdf(v)\n",
        "\n",
        "  return F_dis(x1_new, x2_new, E, gaussians)\n",
        "import numpy as np\n",
        "#check if\n",
        "dists = []\n",
        "for _ in range(500):\n",
        "  x1 = (torch.rand(1) * 6.0 - 3.0).item()  # Sample from [-4.0, 5.0]\n",
        "  x2 = (torch.rand(1) * 6.0 - 3.0).item()\n",
        "  dists.append(torch.abs(fr_sklar(x1, x2, E, gaussians) - F_dis(x1, x2, E_ipfp, gaussians)))\n",
        "print(np.array(dists).mean(), np.median(np.array(dists)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTaGrDcC-cBH",
        "outputId": "42e8b32f-dbec-447b-acce-bd3dbcfa0b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.028965749 0.021127956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def numerical_pdf(cdf_fn, x1, x2, delta, *args):\n",
        "    # Approximate ∂²F/∂x1∂x2 via central difference\n",
        "    fpp = cdf_fn(x1 + delta, x2 + delta, *args)\n",
        "    fpm = cdf_fn(x1 + delta, x2 - delta, *args)\n",
        "    fmp = cdf_fn(x1 - delta, x2 + delta, *args)\n",
        "    fmm = cdf_fn(x1 - delta, x2 - delta, *args)\n",
        "    return (fpp - fpm - fmp + fmm) / (4 * delta**2)\n",
        "\n",
        "def estimate_kl(fr_sklar, F_dis, E, E_ipfp, gaussians,\n",
        "                x_min=-9, x_max=9, steps=50, delta=1e-2):\n",
        "    x_vals = torch.linspace(x_min, x_max, steps)\n",
        "    y_vals = torch.linspace(x_min, x_max, steps)\n",
        "    dx = x_vals[1] - x_vals[0]\n",
        "    dy = y_vals[1] - y_vals[0]\n",
        "    kl = 0.0\n",
        "    eps = 1e-8\n",
        "\n",
        "    for x1 in x_vals:\n",
        "        for x2 in y_vals:\n",
        "            p = numerical_pdf(fr_sklar, x1.item(), x2.item(), delta, E, gaussians).item()\n",
        "            q = numerical_pdf(F_dis, x1.item(), x2.item(), delta, E_ipfp, gaussians).item()\n",
        "            p = max(p, eps)\n",
        "            q = max(q, eps)\n",
        "\n",
        "            #if p <= 0 or q <= 0:\n",
        "            #  print(f\"Invalid PDF: p={p}, q={q}, at x1={x1}, x2={x2}\")\n",
        "            kl += p * np.log(p / (q + eps)) * dx * dy\n",
        "\n",
        "    return kl\n",
        "\n",
        "kl_value = estimate_kl(fr_sklar, F_dis, E, E_ipfp, gaussians)\n",
        "print(f\"Estimated KL divergence: {kl_value:.6f}\")\n"
      ],
      "metadata": {
        "id": "gLQsVKui-ccC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimated KL divergence: -0.040498, 50 steps, on range -9 to 9\n",
        "#Estimated KL divergence: -0.040350"
      ],
      "metadata": {
        "id": "s3FKrnaP-lUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}